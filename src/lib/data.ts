import { Project } from '@/types';

export const projects: Project[] = [
    {
        id: 'portfolio-v1',
        title: 'Personal Portfolio',
        category: 'web-dev',
        description: 'A modern, responsive portfolio website built with Next.js and TailwindCSS.',
        longDescription: 'This project is a personal portfolio website designed to showcase my skills and projects as a software engineer. It features a clean, modern design with a focus on user experience and performance. The site is built using Next.js 15 and TailwindCSS v4, utilizing the latest features of both frameworks.',
        technologies: ['Next.js', 'TailwindCSS', 'TypeScript', 'Framer Motion'],
        github: 'https://github.com/Bruce350-ship-it/portfolio',
        images: ['/assets/screenshots/portfolio.png'],
        problems: 'Ensuring a consistent design across all devices while maintaining high performance.',
        solutions: 'Used TailwindCSS for responsive design and Next.js image optimization.',
        learned: 'Deepened understanding of Next.js App Router and Tailwind v4 configuration.',
        featured: false,
    },
    {
        id: 'saas-multi-tenant-v1',
        title: 'SaaS Multi-Tenant System',
        category: 'web-dev',
        description: 'A comprehensive multi-tenant SaaS platform with automated tenant provisioning, offline-first mobile app, and role-based access control.',
        longDescription: 'A production-ready SaaS multi-tenant system designed for scalable business management. Features include automated tenant provisioning with one-click database setup, separate PostgreSQL databases per tenant for complete data isolation, and an offline-first Flutter mobile application with automatic sync. The platform provides robust role-based authentication (Super Admin, Admin, User) with JWT tokens, comprehensive CRUD operations for users, products, and sales, and real-time mobile synchronization with conflict resolution.',
        technologies: [
            'Next.js 16',
            'PostgreSQL',
            'Flutter 3.0+',
            'React 19',
            'TypeScript',
            'Prisma ORM',
            'TailwindCSS v4',
            'Mantine UI v7',
            'SQLite',
            'JWT Authentication',
            'Node.js 18+'
        ],
        github: 'https://github.com/Bruce350-ship-it/saas-system',
        images: [
            '/assets/projects/SaaS/Screenshot (122).png',
            '/assets/projects/SaaS/Screenshot (87).png',
            '/assets/projects/SaaS/Screenshot (89).png',
            '/assets/projects/SaaS/Screenshot (90).png',
            '/assets/projects/SaaS/Screenshot (91).png',
            '/assets/projects/SaaS/Screenshot (92).png',
            '/assets/projects/SaaS/Screenshot (93).png',
            '/assets/projects/SaaS/Screenshot (94).png',
            '/assets/projects/SaaS/Screenshot (95).png',
            '/assets/projects/SaaS/Screenshot (96).png',
            '/assets/projects/SaaS/IMG-20251124-WA0018.jpg',
            '/assets/projects/SaaS/IMG-20251124-WA0019.jpg',
            '/assets/projects/SaaS/IMG-20251124-WA0020.jpg',
            '/assets/projects/SaaS/IMG-20251124-WA0021.jpg',
            '/assets/projects/SaaS/Screenshot (97).png',
        ],
        problems: 'Building a scalable multi-tenant architecture with complete tenant isolation, implementing offline-first mobile capabilities with automatic sync, ensuring type safety across full-stack with proper JSON serialization, and managing complex database provisioning with automated migrations.',
        solutions: 'Implemented separate PostgreSQL databases per tenant with Prisma ORM for schema management. Created automated tenant provisioning with one-click database creation, migration, and seeding. Built queue-based offline sync system with connectivity monitoring and conflict resolution (server wins). Used TypeScript strict mode with custom JSON converters for type safety across the stack.',
        learned: 'Advanced multi-tenancy patterns with database-per-tenant architecture, complex automated provisioning workflows, offline-first mobile architecture with SQLite and auto-sync, Prisma schema management across multiple databases, JWT-based authentication with role hierarchy, proper error handling and user feedback patterns, and Flutter state management with Provider pattern.',
        featured: true,
    },
    {
        id: 'automl-unified-platform',
        title: 'Unified ML Platform',
        category: 'data-science',
        description: 'An integrated machine learning platform combining Jupyter-style notebooks, automated ML training, and model deployment for predictions - all in a single application.',
        longDescription: 'A comprehensive end-to-end machine learning platform that streamlines the entire ML workflow from data exploration to production deployment. Features include an interactive Jupyter-style notebook with real-time code execution and visualization support, automated ML training with intelligent model selection and hyperparameter recommendations, and a seamless prediction module for deploying trained models. The platform provides automated data preprocessing with smart imputation, scaling, and encoding strategies, supports multiple ML algorithms (Random Forest, Gradient Boosting, Logistic Regression), and includes complete pipeline management with automatic scaler/encoder extraction for production use.',
        technologies: [
            'FastAPI',
            'WebSockets',
            'scikit-learn',
            'Zustand',
            'Python 3.11',
            'pandas',
            'matplotlib',
            'Next.js 15',
            'React 19',
            'TypeScript',
            'TailwindCSS',
            'joblib',
            'Node.js 20',
        ],
        github: 'https://github.com/Bruce350-ship-it/auto-ml',
        images: [
            '/assets/projects/Auto ML/Screenshot (125).png',
            '/assets/projects/Auto ML/Screenshot (127).png',
            '/assets/projects/Auto ML/Screenshot (128).png',
            '/assets/projects/Auto ML/Screenshot (129).png',
            '/assets/projects/Auto ML/Screenshot (130).png',
            '/assets/projects/Auto ML/2660fd0aa79741fc81fd5b4ea05f8b2a.jpg',
            '/assets/projects/Auto ML/Screenshot (131).png',
            '/assets/projects/Auto ML/7939896ea66e466a9c05c5b7450614d0.jpg',
        ],
        videos: [
            '/assets/projects/Auto ML/1125(1).mp4'
        ],
        problems: 'Bridging the gap between data exploration and production ML required three separate tools (Jupyter, AutoML platforms, deployment systems). Ensuring type-safe real-time communication between frontend and backend for live training updates. Managing complex ML pipeline serialization with separate preprocessing artifacts. Implementing Jupyter-like automatic expression evaluation without full kernel overhead.',
        solutions: 'Built unified platform with three integrated modules sharing state via Zustand. Implemented WebSocket-based real-time communication for notebook execution and training progress. Created automatic pipeline artifact extraction - separating scalers and encoders from trained models for flexible prediction deployment. Developed custom code execution engine that evaluates last expressions automatically and captures matplotlib plots as base64 images. Added smart model naming with dataset, target, and timestamp for easy identification.',
        learned: 'Advanced WebSocket patterns for real-time ML training feedback, sklearn pipeline architecture with custom preprocessors, automatic artifact extraction from trained pipelines, implementing Jupyter-like REPL behavior in web environment, FastAPI async patterns for long-running ML tasks, managing multiple ML model lifecycles with proper serialization, and building intuitive ML workflows that guide users from exploration to production.',
        featured: true,
    },
    {
        id: 'housing-price-prediction',
        title: 'Housing Price Prediction',
        category: 'data-science',
        description: 'End-to-end ML project with automated training, tuning, and serving pipelines.',
        longDescription: 'A comprehensive MLOps pipeline for housing price prediction. Features include data cleaning, automated model training with Optuna hyperparameter tuning (Gradient Boosting, Random Forest, LightGBM), experiment tracking with MLflow, and a production-ready FastAPI inference service. Includes drift monitoring and is fully containerized with Docker.',
        technologies: ['Scikit-learn', 'Optuna', 'MLflow', 'FastAPI', 'LightGBM', 'Python', 'Docker', 'Pandas'],
        github: 'https://github.com/Bruce350-ship-it/housing',
        images: [
            '/assets/projects/Housing/dataset-card.jpg',
            '/assets/projects/Housing/area_vs_price.png',
            '/assets/projects/Housing/conditioning_vs_price.png',
            '/assets/projects/Housing/corr_heatmap.png',
            '/assets/projects/Housing/num_distributions.png',
            '/assets/projects/Housing/num_pairplot.png',
        ],
        problems: 'Managing the full lifecycle of machine learning models from raw data to production deployment, including consistent experiment tracking and ensuring reproducibility across environments.',
        solutions: 'Implemented a modular pipeline with Optuna for automated hyperparameter tuning and MLflow for experiment tracking. Containerized the entire workflow using Docker to ensure consistent execution and easy deployment of the FastAPI inference service.',
        learned: 'Gained hands-on experience with MLOps best practices, including automated model selection, artifact management with MLflow, and building scalable inference APIs with FastAPI.',
        featured: false,
    },
    {
        "id": "mining-erp-v1",
        "title": "Mining ERP Platform",
        "category": "web-dev",
        "description": "A modern, modular ERP web application for mining operations, built with Next.js and TailwindCSS.",
        "longDescription": "This project is a full-featured ERP platform tailored for mining companies. It manages key business domains including environment, finance, HR, inventory, maintenance, operations, payroll, procurement, and safety. The application leverages Next.js (App Router), TypeScript, and TailwindCSS for a scalable, maintainable, and responsive user experience. Prisma is used for database access, and the UI is enhanced with custom charts and analytics.",
        "technologies": ["Next.js", "Prisma", "TailwindCSS", "TypeScript", "React Charts"],
        "github": "https://github.com/Bruce350-ship-it/mining-erp",
        "images": [
            '/assets/projects/mining erp/Screenshot (132).png',
            '/assets/projects/mining erp/Screenshot (133).png',
            '/assets/projects/mining erp/Screenshot (134).png',
            '/assets/projects/mining erp/Screenshot (135).png',
            '/assets/projects/mining erp/Screenshot (136).png',
            '/assets/projects/mining erp/Screenshot (137).png',
            '/assets/projects/mining erp/Screenshot (138).png',
            '/assets/projects/mining erp/Screenshot (139).png',
        ],
        "videos": [
            '/assets/projects/mining erp/mining_erp.mp4'
        ],
        "problems": "Integrating diverse business modules while maintaining a unified user experience and robust data integrity.",
        "solutions": "Used modular Next.js architecture, Prisma for consistent data access, and TailwindCSS for responsive design. Custom chart components provide actionable analytics.",
        "learned": "Gained expertise in building scalable Next.js applications, advanced TailwindCSS layouts, and integrating Prisma with complex business logic.",
        "featured": true
    },
    {
        id: 'btc-forecast-v1',
        title: 'Bitcoin Price Forecasting & Direction Classification',
        category: 'data-science',
        description: 'A machine learning project for forecasting Bitcoin prices and classifying market direction using deep learning models.',
        longDescription: 'This project analyzes historical Bitcoin data to train and deploy deep learning models for price forecasting and direction classification. It features Jupyter notebooks for data exploration and model development, a Flask API for serving predictions, and scripts for live inference using real-time data from CoinGecko. The models are built with Keras and use feature scaling for improved accuracy.',
        technologies: ['Keras', 'Flask', 'pycoingecko', 'scikit-learn', 'Python', 'pandas', 'numpy', 'matplotlib', 'Jupyter Notebook'],
        "github": "https://github.com/Bruce350-ship-it/Data-Science-Projects/tree/main/BTC%20Forecast%20Project",
        "images": [
            '/assets/projects/btc_forecast/btc.jpg',
            '/assets/projects/btc_forecast/btc_2.jpg',
            '/assets/projects/btc_forecast/Screenshot (140).png',
            '/assets/projects/btc_forecast/Screenshot (141).png',
            '/assets/projects/btc_forecast/Screenshot (142).png',
            '/assets/projects/btc_forecast/Screenshot (143).png',
            '/assets/projects/btc_forecast/Screenshot (144).png',
        ],
        problems: 'Predicting volatile cryptocurrency prices and market direction; integrating live data for real-time inference.',
        solutions: 'Used LSTM-based deep learning models, feature engineering, and robust data preprocessing; deployed models via Flask API and automated live data fetching.',
        learned: 'Improved skills in time series forecasting, classification, model deployment, and working with financial APIs.',
        featured: false
    },
    {
        "id": "web-logs-ds-v1",
        "title": "Web Logs Data Science Analysis",
        "category": "data-science",
        "description": "A scalable ETL and analysis pipeline for web server logs using PySpark and Python visualization.",
        "longDescription": "This project processes and analyzes web server log data using PySpark for scalable ETL and Python libraries for visualization. It cleans raw logs, engineers features, detects bots and suspicious activity, and exports a cleaned dataset for further analysis. The workflow demonstrates best practices in big data processing and exploratory data analysis.",
        "technologies": ["PySpark", "pandas", "matplotlib", "seaborn"],
        "github": "https://github.com/Bruce350-ship-it/Data-Science-Projects/tree/main/Web_Logs%20Project",
        "images": [
            '/assets/projects/web_logs/web_logs.jpg',
            '/assets/projects/web_logs/Screenshot (145).png',
            '/assets/projects/web_logs/Screenshot (146).png',
            '/assets/projects/web_logs/Screenshot (147).png',
        ],
        "problems": "Handling large, messy log files and extracting meaningful features for analysis and anomaly detection.",
        "solutions": "Used PySpark for scalable data cleaning and transformation, and Python libraries for visualization and feature engineering.",
        "learned": "Gained experience in big data ETL, feature engineering for web analytics, and scalable log analysis with Spark.",
        "featured": false
    },
    {
        "id": "nlp-spacy-intent-resume-v1",
        "title": "NLP Resume Parser & Intent Classifier",
        "category": "data-science",
        "description": "A Jupyter Notebook project for extracting structured data from resumes and building a custom intent classifier using spaCy.",
        "longDescription": "This project demonstrates two practical NLP applications using spaCy: parsing resumes to extract structured information and building a custom intent classification model for user queries. It showcases entity recognition, regex-based extraction, and text categorization, all implemented in Python within a Jupyter Notebook.",
        "technologies": ["Python", "Jupyter Notebook", "spaCy", "pandas", "Regex"],
        "github": "https://github.com/Bruce350-ship-it/Data-Science-Projects/blob/main/NLP%20with%20spaCy.ipynb",
        "images": [
            '/assets/projects/Natural-Language-Processing.jpg',
        ],
        "problems": "Extracting structured information from unstructured text and accurately classifying user intents.",
        "solutions": "Used spaCy's NER and regex for data extraction; built and trained a spaCy text categorizer for intent classification.",
        "learned": "Gained experience with spaCy pipelines, entity recognition, text classification, and practical NLP workflows.",
        "featured": false
    },
    {
        id: 'bike-buyers-analytics-v1',
        title: 'Bike Buyers Analytics Dashboard',
        category: 'data-science',
        description: 'An Excel-based analytics project that visualizes customer demographics and bike purchase behavior using pivot tables and interactive charts.',
        longDescription: 'This project is a comprehensive data analytics dashboard built in Excel. It analyzes a dataset of customer demographics, socioeconomic factors, and behavioral attributes to understand what influences bike purchasing decisions. The project includes cleansed datasets, pivot tables, and visually compelling charts that enable insights into age groups, income levels, regions, commute distances, and more. It serves as a practical example of Excel BI reporting and dashboard creation.',
        technologies: ['Microsoft Excel', 'Pivot Tables', 'Data Visualization', 'Business Intelligence'],
        github: "https://github.com/Bruce350-ship-it/Excel-Projects/blob/main/Excel%20Project%20Dataset.xlsx",
        images: [
            '/assets/projects/bicycle_sales/bicycle_sales.jpg',
            '/assets/projects/bicycle_sales/Screenshot (148).png',
            '/assets/projects/bicycle_sales/Screenshot (149).png',
        ],
        problems: 'Interpreting multi-dimensional demographic data and presenting it in a clear, insightful format for decision-making.',
        solutions: 'Used structured Excel tables, pivot tables, slicers, and dynamic charts to present insights clearly and interactively.',
        learned: 'Gained deeper experience in Excel analytics workflows, including pivot operations, data cleaning, and dashboard design for business insights.',
        featured: false
    },
    {
        id: 'vg-sales-dashboard-v1',
        title: 'Video Game Sales Analytics Dashboard',
        category: 'data-science',
        description: 'A fully interactive Power BI dashboard analyzing historical video game sales across regions, platforms, genres, and publishers.',
        longDescription: 'This project is a comprehensive business intelligence dashboard built in Power BI to visualize and analyze global video game sales data. It features multiple report pages with interactive charts, slicers, and KPIs that help users explore trends by year, platform, genre, region, and publisher. The model uses DAX measures and Power Query transformations to deliver clean, accurate insights.',
        technologies: ['Power BI', 'DAX', 'Power Query (M)', 'Data Modeling'],
        github: "https://github.com/Bruce350-ship-it/PowerBI-Projects/blob/main/vg_sales.pbix",
        images: [
            '/assets/projects/vg_sales/vg_sales.jpg',
            '/assets/projects/vg_sales/Screenshot (150).png',
        ],
        problems: 'Raw dataset required significant cleaning and normalization, including inconsistent platform labels and missing regional data.',
        solutions: 'Used Power Query for data transformation and applied DAX measures for accurate aggregations and comparisons across dimensions.',
        learned: 'Improved skills in Power BI data modeling, DAX calculations, visualization best practices, and dashboard performance optimization.',
        featured: false
    },
    {
        id: 'sql-employee-analytics-v1',
        title: 'Employee Data SQL Analytics Project',
        category: 'data-science',
        description: 'A structured SQL project demonstrating data querying, analytics, and database concepts using an employee management dataset.',
        longDescription: 'This project showcases a comprehensive set of SQL techniques using Microsoft SQL Server. It includes joins, conditional logic, ranking with window functions, subqueries, views for BI tools, temporary tables for ETL-style workflows, CTEs for summarization, and stored procedures for reusable data retrieval. The SQL script simulates common real-world employee and payroll analytics tasks.',
        technologies: ['SQL Server', 'T-SQL', 'SSMS', 'Database Design'],
        github: "https://github.com/Bruce350-ship-it/SQL-Project-1/blob/main/SQL_Project.sql",
        images: [
            '/assets/projects/employee_analytics.jpg',
        ],
        problems: 'Needed a clean and flexible way to analyze employee data and prepare it for reporting tools.',
        solutions: 'Used SQL views, CTEs, and temporary tables to simplify transformations and provide structured outputs for analytics.',
        learned: 'Improved understanding of SQL Server window functions, procedural SQL, and best practices for building analytics-ready data models.',
        featured: false
    },
];
